# nodes/retrieve_evidence.py
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5")

def retrieve_evidence(state):
    """
    –£–∑–µ–ª: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ FAISS.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç get_unique_union –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """
    print("üîé –£–∑–µ–ª: –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º...")
    
    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –æ—Ç multi_query
    queries = state.get("queries", [])
    chunks_data = state.get("chunks_with_metadata", [])
    
    if not queries or not chunks_data:  # ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –±—ã–ª–æ `chunks_`
        print("‚ö†Ô∏è –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ —á–∞–Ω–∫–æ–≤.")
        return {"evidence": []}

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è FAISS
    texts = [chunk["text"] for chunk in chunks_data]
    metadatas = [chunk["metadata"] for chunk in chunks_data]

    # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # –§—É–Ω–∫—Ü–∏—è: —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    def get_unique_union(doc_lists):
        seen = set()
        unique_docs = []
        for docs in doc_lists:
            for doc in docs:
                content_hash = hash(doc.page_content[:100])
                if content_hash not in seen:
                    seen.add(content_hash)
                    unique_docs.append(doc)
        return unique_docs

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    all_docs = []
    for query in queries:
        print(f"üîç –ü–æ–∏—Å–∫: '{query}'")
        docs = retriever.invoke(query)
        all_docs.append(docs)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    unique_docs = get_unique_union(all_docs)

    # –§–æ—Ä–º–∏—Ä—É–µ–º evidence
    evidence = []
    for query in queries:
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–∫—É: "–ù–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É: ..."
        pass

    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ –∫–∞–∫ –æ—Å–Ω–æ–≤—É
    evidence_items = []
    for i, doc in enumerate(unique_docs):
        evidence_items.append({
            "hypothesis": f"Relevant fragment (query-translated) {i+1}",
            "chunks": [{"text": doc.page_content}]
        })

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    return {"evidence": evidence_items}