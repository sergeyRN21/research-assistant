# app.py
import streamlit as st
from dotenv import load_dotenv
load_dotenv()

from graph import app
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# üöÄ –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ ‚Äî —Å–æ–∑–¥–∞—ë—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5")
vectorstore = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∑–∂–µ

st.set_page_config(page_title="üß† Research Assistant", layout="wide")
st.title("üß† Research Assistant ‚Äî –ù–∞—É—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏")

question = st.text_input("–í–≤–µ–¥–∏—Ç–µ –Ω–∞—É—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å:", placeholder="–ö–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã —Å–Ω–∏–∂–∞—é—Ç KV-cache?")

# app.py (–≤–Ω—É—Ç—Ä–∏ –∫–Ω–æ–ø–∫–∏ "–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑")
if st.button("üîç –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑"):
    if not question.strip():
        st.error("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å!")
    else:
        initial_state = {
            "question": question,
            "papers": [],
            "chunks_with_metadata": [],
            "hypotheses": [],
            "evidence": [],
            "final_answer": "",
            "retry_count": 0,
            "error": ""
        }

        with st.spinner("üöÄ –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è..."):
            try:
                # 1. –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏
                result = app.invoke(initial_state, config={"recursion_limit": 10})
                chunks_data = result.get("chunks_with_metadata", [])

                # 2. –ï—Å–ª–∏ –µ—Å—Ç—å —á–∞–Ω–∫–∏ ‚Äî —Å–æ–∑–¥–∞–µ–º vectorstore
                if chunks_data:
                    texts = [chunk["text"] for chunk in chunks_data]
                    metadatas = [chunk.get("metadata", {}) for chunk in chunks_data]

                    # üöÄ –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –æ–¥–∏–Ω —Ä–∞–∑
                    global_vectorstore = FAISS.from_texts(
                        texts=texts,
                        embedding=embedding_model,
                        metadatas=metadatas
                    )

                    # 3. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –µ–≥–æ –≤ graph
                    from graph import set_global_vectorstore
                    set_global_vectorstore(global_vectorstore)

                    # 4. –ü–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ–º –≥—Ä–∞—Ñ ‚Äî —Ç–µ–ø–µ—Ä—å —Å vectorstore
                    final_state = app.invoke(result, config={"recursion_limit": 10})

                else:
                    final_state = result

                st.success("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!")
                st.markdown("### üìù –û—Ç–≤–µ—Ç")
                st.markdown(final_state["final_answer"])

                st.markdown("### üîó –¶–µ–ø–æ—á–∫–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤")
                for item in final_state.get("evidence", []):
                    hyp = item.get("hypothesis", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                    with st.expander(f"–ì–∏–ø–æ—Ç–µ–∑–∞: {hyp}"):
                        for vc in item.get("validated_chunks", []):
                            j = vc["judgment"]
                            status = "‚úÖ –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ" if j["confirmed"] else ("üü° –ß–∞—Å—Ç–∏—á–Ω–æ" if j["partial"] else "‚ùå –ù–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ")
                            st.markdown(f"""
                            - **–°—Ç–∞—Ç—É—Å**: {status}
                            - **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å**: {j['confidence']:.2f}
                            - **–ü—Ä–∏—á–∏–Ω–∞**: {j['reason']}
                            - **–§—Ä–∞–≥–º–µ–Ω—Ç**: *{vc['text'][:300]}...*
                            """)

            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")