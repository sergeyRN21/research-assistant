# nodes/extract_text.py
import requests
from pypdf import PdfReader
import io
import re
from langchain_text_splitters import RecursiveCharacterTextSplitter

def extract_text(state):
    """
    –£–∑–µ–ª 2: –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
    """
    print("üìÑ –£–∑–µ–ª: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF + chunking —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏...")
    
    papers = state.get("papers", [])
    if not papers:
        print("‚ö†Ô∏è –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.")
        return {"retrieved_texts": []}
    
    all_chunks_with_metadata = []
    for i, paper in enumerate(papers):
        pdf_url = paper.get("pdf_url")
        if not pdf_url:
            continue

        print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º PDF [{i+1}/{len(papers)}]: {pdf_url}")
        try:
            response = requests.get(pdf_url, timeout=15)
            response.raise_for_status()

            pdf = PdfReader(io.BytesIO(response.content))
            full_text = ""
            for page in pdf.pages:
                full_text += page.extract_text() + "\n"
            
            # üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è
            clean_text = re.sub(r'\s+', ' ', full_text).strip()
            structure_keywords = ["abstract", "introduction", "method", "experiment", "results", "conclusion"]
            if not any(kw in clean_text.lower()[:1000] for kw in structure_keywords):
                print("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: –Ω–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–∏")
                continue

            tech_terms = ["attention", "kv cache", "quantization", "layer", "embedding", "model", "inference"]
            if not any(term in clean_text.lower() for term in tech_terms):
                print("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: –Ω–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è")
                continue

            # üî• –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            splitter = RecursiveCharacterTextSplitter(
                separators=["\n\n", "\n", ".", " ", ""],
                chunk_size=500,
                chunk_overlap=100
            )
            chunks = splitter.split_text(full_text)

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
            for chunk in chunks:
                metadata = {
                    "source_title": paper["title"],
                    "contains_method": any(kw in chunk.lower() for kw in ["method", "algorithm", "approach"]),
                    "contains_results": any(kw in chunk.lower() for kw in ["result", "accuracy", "throughput", "memory", "table", "figure"]),
                    "contains_experiment": any(kw in chunk.lower() for kw in ["experiment", "benchmark", "evaluation", "dataset"]),
                    "contains_figures": "figure" in chunk.lower() or "table" in chunk.lower(),
                    "page_estimate": len(full_text[:full_text.find(chunk)]) // 2000  # –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                }
                all_chunks_with_metadata.append({
                    "text": chunk,
                    "metadata": metadata
                })

            print(f"‚úÖ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pdf_url}: {e}")
    
    print(f"‚úÖ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏: {len(all_chunks_with_metadata)}")
    return {"chunks_with_metadata": all_chunks_with_metadata}