# nodes/retrieve_evidence.py
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5")

def retrieve_evidence(state):
    """
    –£–∑–µ–ª 4: –î–ª—è –∫–∞–∂–¥–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç FAISS –≤–º–µ—Å—Ç–æ Chroma ‚Äî –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ Streamlit Cloud.
    
    –í—Ö–æ–¥: state["hypotheses"], state["retrieved_texts"]
    –í—ã—Ö–æ–¥: state["evidence"] = [{"hypothesis": "...", "chunks": [...]}]
    """
    print("üîé –£–∑–µ–ª: –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ (FAISS)...")
    
    hypotheses = state.get("hypotheses", [])
    texts = state.get("retrieved_texts", [])
    
    if not hypotheses or not texts:
        print("‚ö†Ô∏è –ù–µ—Ç –≥–∏–ø–æ—Ç–µ–∑ –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞.")
        return {"evidence": []}
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = []
    for text in texts:
        if text.strip():
            chunks.extend(splitter.split_text(text))
    
    if not chunks:
        print("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è.")
        return {"evidence": []}
    
    print(f"üìÑ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
    
    try:
        # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ –ø–∞–º—è—Ç–∏
        vectorstore = FAISS.from_texts(
            texts=chunks,
            embedding=embedding_model
        )
        
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        
        evidence = []
        for hypothesis in hypotheses:
            print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –≥–∏–ø–æ—Ç–µ–∑–µ: '{hypothesis[:60]}...'")
            docs = retriever.invoke(hypothesis)
            found_chunks = [{"text": doc.page_content} for doc in docs]
            
            evidence.append({
                "hypothesis": hypothesis,
                "chunks": found_chunks
            })
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–ª—è {len(evidence)} –≥–∏–ø–æ—Ç–µ–∑")
        return {"evidence": evidence}
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤: {e}")
        return {"evidence": [], "error": str(e)}