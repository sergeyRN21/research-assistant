# nodes/retrieve_evidence.py
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os
from pathlib import Path

# –ö—ç—à –¥–ª—è FAISS
FAISS_CACHE_DIR = Path("cache/faiss")
FAISS_CACHE_DIR.mkdir(exist_ok=True)

embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5")

def retrieve_evidence(state):
    """
    –£–∑–µ–ª 4: –î–ª—è –∫–∞–∂–¥–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç FAISS —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    """
    print("üîé –£–∑–µ–ª: –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ (FAISS + –∫—ç—à)...")
    
    hypotheses = state.get("hypotheses", [])
    texts = state.get("retrieved_texts", [])
    
    if not hypotheses or not texts:
        print("‚ö†Ô∏è –ù–µ—Ç –≥–∏–ø–æ—Ç–µ–∑ –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞.")
        return {"evidence": []}
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
    chunks = []
    for text in texts:
        if text.strip():
            chunks.extend(splitter.split_text(text))
    
    if not chunks:
        print("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è.")
        return {"evidence": []}

    # üî• –ö—ç—à–∏—Ä—É–µ–º FAISS –ø–æ —Ö–µ—à—É —Ç–µ–∫—Å—Ç–æ–≤
    import hashlib
    cache_key = hashlib.md5("".join(chunks).encode()).hexdigest()
    cache_path = FAISS_CACHE_DIR / cache_key

    if (cache_path / "index.faiss").exists():
        print("üíæ –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π FAISS –∏–Ω–¥–µ–∫—Å...")
        vectorstore = FAISS.load_local(
            str(cache_path),
            embedding_model,
            allow_dangerous_deserialization=True
        )
    else:
        print("üèóÔ∏è –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π FAISS –∏–Ω–¥–µ–∫—Å...")
        vectorstore = FAISS.from_texts(texts=chunks, embedding=embedding_model)
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å –≤ –∫—ç—à...")
        vectorstore.save_local(cache_path)

    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    evidence = []
    for hypothesis in hypotheses:
        print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –≥–∏–ø–æ—Ç–µ–∑–µ: '{hypothesis[:60]}...'")
        docs = retriever.invoke(hypothesis)
        found_chunks = [{"text": doc.page_content} for doc in docs]
        
        evidence.append({
            "hypothesis": hypothesis,
            "chunks": found_chunks
        })

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–ª—è {len(evidence)} –≥–∏–ø–æ—Ç–µ–∑")
    return {"evidence": evidence}