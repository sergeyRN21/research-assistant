# nodes/retrieve_evidence.py
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5")

def retrieve_evidence(state):
    """
    –£–∑–µ–ª 4: –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ FAISS.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ª—å–∫–æ –≥–¥–µ –µ—Å—Ç—å 'results').
    """
    print("üîé –£–∑–µ–ª: –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º...")

    queries = state.get("queries", [])
    chunks_data = state.get("chunks_with_metadata", [])

    if not queries or not chunks_data:
        print("‚ö†Ô∏è –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ —á–∞–Ω–∫–æ–≤.")
        return {"evidence": []}

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è FAISS
    texts = [chunk["text"] for chunk in chunks_data]
    metadatas = [chunk["metadata"] for chunk in chunks_data]

    vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)
    
    # –§—É–Ω–∫—Ü–∏—è: —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    def get_unique_union(docs_list):
        seen = set()
        unique_docs = []
        for doc in docs_list:
            content_hash = hash(doc.page_content[:200])
            if content_hash not in seen:
                seen.add(content_hash)
                unique_docs.append(doc)
        return unique_docs

    evidence_items = []
    for query in queries:
        print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query[:60]}...'")

        # üî• –ò—â–µ–º —Ç–æ–ª—å–∫–æ –≤ —á–∞–Ω–∫–∞—Ö —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–ª–∏ –º–µ—Ç–æ–¥–∞–º–∏
        docs = vectorstore.similarity_search_with_score(
            query,
            k=3,
            filter=lambda m: m.get("contains_results", False) or m.get("contains_method", False)
        )
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_docs = get_unique_union([doc for doc, _ in docs])

        found_chunks = []
        for doc in unique_docs:
            found_chunks.append({
                "text": doc.page_content,
                "metadata": doc.metadata
            })

        evidence_items.append({
            "hypothesis": f"Relevant fragment (query-translated): {query}",  # –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≥–∏–ø–æ—Ç–µ–∑
            "chunks": found_chunks
        })

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–ª—è {len(evidence_items)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    return {"evidence": evidence_items}