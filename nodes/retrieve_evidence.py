# nodes/retrieve_evidence.py
from langchain_community.vectorstores import FAISS
from nodes.embedding_loader import get_embedding_model

def retrieve_evidence(state):
    print("üîé –£–∑–µ–ª: –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º...")

    queries = state.get("queries", [])
    chunks_data = state.get("chunks_with_metadata", [])

    if not queries or not chunks_data:
        print("‚ö†Ô∏è –ù–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ —á–∞–Ω–∫–æ–≤.")
        return {"evidence": []}

    # üî• Lazy load —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ‚Äî —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ
    embedding_model = get_embedding_model()

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è FAISS
    texts = [chunk["text"] for chunk in chunks_data]
    metadatas = [chunk.get("metadata", {}) for chunk in chunks_data]

    # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 1})

    # –§—É–Ω–∫—Ü–∏—è: —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    def get_unique_union(doc_lists):
        seen = set()
        unique_docs = []
        for docs in doc_lists:
            for doc in docs:
                content_hash = hash(doc.page_content[:100])
                if content_hash not in seen:
                    seen.add(content_hash)
                    unique_docs.append(doc)
        return unique_docs

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    all_docs = []
    for query in queries:
        print(f"üîç –ü–æ–∏—Å–∫: '{query}'")
        docs = retriever.invoke(query)
        all_docs.append(docs)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    unique_docs = get_unique_union(all_docs)

    # –§–æ—Ä–º–∏—Ä—É–µ–º evidence
    evidence_items = []
    for i, doc in enumerate(unique_docs):
        evidence_items.append({
            "hypothesis": f"Relevant fragment (query-translated) {i+1}",
            "chunks": [{
                "text": doc.page_content,
                "metadata": getattr(doc, "metadata", {})
            }]
        })

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    return {"evidence": evidence_items}