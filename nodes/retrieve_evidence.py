# nodes/retrieve_evidence.py
from langchain_community.vectorstores import FAISS
# nodes/retrieve_evidence.py (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º embedding_loader)
from .embedding_loader import get_embedding_model # –ò–º–ø–æ—Ä—Ç –∏–∑ —Å–æ—Å–µ–¥–Ω–µ–≥–æ —Ñ–∞–π–ª–∞

# embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5") # –£–¥–∞–ª–∏—Ç—å —ç—Ç—É —Å—Ç—Ä–æ–∫—É
embedding_model = get_embedding_model() # –ó–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é

# ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –æ—Å—Ç–∞—ë—Ç—Å—è —Ç–µ–º –∂–µ

def retrieve_evidence(state):
    """
    –£–∑–µ–ª: –î–ª—è –∫–∞–∂–¥–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã –∏–∑ `state["hypotheses"]` –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç multi-query –∏–∑ `state["queries"]` –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É –≥–∏–ø–æ—Ç–µ–∑–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É.
    """
    print("üîé –£–∑–µ–ª: –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ø–æ –≥–∏–ø–æ—Ç–µ–∑–∞–º...")

    hypotheses = state.get("hypotheses", [])
    queries = state.get("queries", []) # Multi-query –≤–∞—Ä–∏–∞–Ω—Ç—ã
    chunks_data = state.get("chunks_with_metadata", [])

    if not hypotheses or not chunks_data:
        print("‚ö†Ô∏è –ù–µ—Ç –≥–∏–ø–æ—Ç–µ–∑ –∏–ª–∏ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤.")
        return {"evidence": []}

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    texts = [chunk["text"] for chunk in chunks_data]
    metadatas = [chunk.get("metadata", {}) for chunk in chunks_data]

    # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # –§—É–Ω–∫—Ü–∏—è: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    def get_unique_union(docs_lists):
        seen = set()
        unique_docs = []
        for docs in docs_lists:
            for doc in docs:
                content_hash = hash(doc.page_content[:100])
                if content_hash not in seen:
                    seen.add(content_hash)
                    unique_docs.append(doc)
        return unique_docs

    evidence_list = []

    # –î–ª—è –∫–∞–∂–¥–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã
    for hypothesis in hypotheses:
        print(f"üîç –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –¥–ª—è –≥–∏–ø–æ—Ç–µ–∑—ã: '{hypothesis}'")
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ + multi-query –≤–∞—Ä–∏–∞–Ω—Ç—ã)
        search_queries = [hypothesis] + queries

        all_docs_for_hyp = []
        for query in search_queries:
            print(f"   üìÑ –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
            docs = retriever.invoke(query)
            all_docs_for_hyp.append(docs)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —ç—Ç–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        unique_docs_for_hyp = get_unique_union(all_docs_for_hyp)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —ç—Ç–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã
        chunks_for_hyp = [{"text": doc.page_content, "metadata": doc.metadata} for doc in unique_docs_for_hyp]

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ evidence
        evidence_list.append({
            "hypothesis": hypothesis,
            "chunks": chunks_for_hyp
        })

    print(f"‚úÖ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(evidence_list)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞.")
    return {"evidence": evidence_list}