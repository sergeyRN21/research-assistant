# nodes/extract_text.py
import requests
from pypdf import PdfReader
import io
import re

def extract_text(state):
    """
    –£–∑–µ–ª 2: –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF.
    –û—Ç—Å–µ–∏–≤–∞–µ—Ç PDF —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, –≥—Ä–∞–Ω—Ç–∞–º–∏, –ª–∏—Ü–µ–Ω–∑–∏—è–º–∏.
    –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ —Å –Ω–∞—É—á–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º.
    """
    print("üìÑ –£–∑–µ–ª: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF...")
    
    papers = state.get("papers", [])
    if not papers:
        print("‚ö†Ô∏è –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.")
        return {"retrieved_texts": []}
    
    retrieved_texts = []
    for i, paper in enumerate(papers):
        pdf_url = paper.get("pdf_url")
        if not pdf_url:
            continue

        print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º PDF [{i+1}/{len(papers)}]: {pdf_url}")
        try:
            response = requests.get(pdf_url, timeout=15)
            response.raise_for_status()

            pdf = PdfReader(io.BytesIO(response.content))
            full_text = ""
            for page in pdf.pages:
                full_text += page.extract_text() + "\n"
            
            # üîç –ß–∏—Å—Ç–∏–º –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            clean_text = re.sub(r'\s+', ' ', full_text).strip()
            
            # ‚ùå –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã "–º—É—Å–æ—Ä–Ω–æ–≥–æ" PDF
            bad_indicators = [
                len(clean_text) < 1000,
                "Personal use of this material is permitted" in clean_text,
                "¬©" in clean_text[:300] and "All rights reserved" in clean_text,
                "This research was supported by" in clean_text,
                "grant" in clean_text[:500] and "funded" in clean_text[:500],
                "IEEE" in clean_text[:200] and "Proceedings" in clean_text[:300],
            ]
            if any(bad_indicators):
                print("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: —ç—Ç–æ –Ω–µ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏")
                retrieved_texts.append("")
                continue
            
            # üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–∞—É—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            structure_keywords = ["abstract", "introduction", "method", "experiment", "results", "conclusion"]
            if not any(kw in clean_text.lower()[:1000] for kw in structure_keywords):
                print("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: –Ω–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–∏")
                retrieved_texts.append("")
                continue
            
            # üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ ML-—Ç–µ—Ä–º–∏–Ω–æ–≤
            tech_terms = ["attention", "kv cache", "quantization", "layer", "embedding", "model", "inference"]
            if not any(term in clean_text.lower() for term in tech_terms):
                print("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º: –Ω–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è")
                retrieved_texts.append("")
                continue
            
            # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10K —Å–∏–º–≤–æ–ª–æ–≤
            text = full_text
            retrieved_texts.append(text)
            print(f"‚úÖ –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á—ë–Ω ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pdf_url}: {e}")
            retrieved_texts.append("")
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(retrieved_texts)}")
    return {"retrieved_texts": retrieved_texts}