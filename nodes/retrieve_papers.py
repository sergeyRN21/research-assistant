# nodes/retrieve_papers.py
import arxiv
import re

def retrieve_papers(state):
    """
    –£–∑–µ–ª 1: –ù–∞—Ö–æ–¥–∏—Ç —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ arXiv API.
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–µ URL –≤–µ–¥—É—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ PDF.
    
    –í—Ö–æ–¥: state["question"]
    –í—ã—Ö–æ–¥: state["papers"]
    """
    print("üîç –£–∑–µ–ª: –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ arXiv API...")
    
    question = state["question"]
    
    search = arxiv.Search(
        query=question,
        max_results=3,  # —É–º–µ–Ω—å—à–∞–µ–º, —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–µ–µ
        sort_by=arxiv.SortCriterion.Relevance
    )
    
    papers = []
    try:
        for result in search.results():
            # üî• –§–æ—Ä–º–∏—Ä—É–µ–º URL –≤—Ä—É—á–Ω—É—é ‚Äî –Ω–∞–¥—ë–∂–Ω–µ–µ
            base_id = result.entry_id.split("/")[-1].split("v")[0]
            pdf_url = f"https://arxiv.org/pdf/{base_id}.pdf"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω
            if not re.match(r"https://arxiv\.org/pdf/[\d\.]+\.pdf", pdf_url):
                print(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –¥–ª—è —Å—Ç–∞—Ç—å–∏: {result.title}")
                continue

            papers.append({
                "entry_id": result.entry_id,
                "title": result.title,
                "summary": result.summary,
                "pdf_url": pdf_url,  # —Ç–µ–ø–µ—Ä—å —Ç–æ—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π
                "published": result.published.strftime("%Y-%m-%d"),
                "authors": [author.name for author in result.authors]
            })
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—Ç–∞—Ç–µ–π: {e}")
        return {"papers": [], "error": str(e)}
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(papers)} —Å—Ç–∞—Ç–µ–π —Å PDF.")
    
    return {"papers": papers}